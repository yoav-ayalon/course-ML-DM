{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282b227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run preprocess_models.ipynb to create preprocessed data (if not already done)\n",
    "%run ../preprocess_models.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40768f83",
   "metadata": {},
   "source": [
    "# Mixture Model\n",
    "\n",
    "This notebook continues from `models.ipynb` and applies a mixture model approach to classify insurance claims.\n",
    "\n",
    "**Prerequisites:** Run `models.ipynb` first to create the preprocessed data and train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e906d880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded preprocessed data from data/processed/\n",
      "\n",
      "Data shape: (9912, 18)\n",
      "Train set: 7929 samples, 17 features\n",
      "Test set: 1983 samples\n",
      "\n",
      "✓ Ready for mixture model!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the preprocessed data from preprocess_models.ipynb\n",
    "# All data is saved in data/processed/ after running preprocess_models.ipynb\n",
    "\n",
    "df = pd.read_csv('../../data/processed/df_preprocessed.csv')\n",
    "X_train = pd.read_csv('../../data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('../../data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('../../data/processed/y_train.csv').squeeze()\n",
    "y_test = pd.read_csv('../../data/processed/y_test.csv').squeeze()\n",
    "\n",
    "print(\"✓ Loaded preprocessed data from data/processed/\")\n",
    "print(f\"\\nData shape: {df.shape}\")\n",
    "print(f\"Train set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\n✓ Ready for mixture model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92dcd6e",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Data for Mixture Model\n",
    "\n",
    "We need to encode all categorical variables and scale numerical features for the GMM to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0cb9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded feature space: 24 features\n",
      "Training set: 7929 samples\n",
      "Test set: 1983 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "# Encode categorical variables\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# print(X_train_encoded.columns)\n",
    "# print(X_test_encoded.columns)\n",
    "\n",
    "# Ensure train and test have the same columns (in case of missing categories)\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "# Standardize features (important for GMM distance calculations)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "print(f\"Encoded feature space: {X_train_encoded.shape[1]} features\")\n",
    "print(f\"Training set: {X_train_scaled.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_scaled.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83b75b",
   "metadata": {},
   "source": [
    "## Step 2: Fit Gaussian Mixture Model\n",
    "\n",
    "We'll start with K=3 components to discover latent risk groups in our population. The GMM will cluster customers based on their feature patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "824e7256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted GMM with 3 components\n",
      "\n",
      "Component sizes in training set:\n",
      "  Component 0: 4001 samples (50.5%)\n",
      "  Component 1: 2331 samples (29.4%)\n",
      "  Component 2: 1597 samples (20.1%)\n",
      "\n",
      "Does the GMM converge? True\n",
      "number of iterations: 4\n"
     ]
    }
   ],
   "source": [
    "# Fit Gaussian Mixture Model with K=3 components\n",
    "n_components = 3\n",
    "gmm = GaussianMixture(\n",
    "    n_components=n_components,\n",
    "    covariance_type='full',  # Allow each component its own full covariance matrix\n",
    "    random_state=42,\n",
    "    n_init=10  # Multiple initializations for stability\n",
    ")\n",
    "\n",
    "gmm.fit(X_train_scaled)\n",
    "\n",
    "# Get component assignments for training data\n",
    "train_component_probs = gmm.predict_proba(X_train_scaled)  # Posterior probabilities\n",
    "train_components = gmm.predict(X_train_scaled)  # Hard assignments\n",
    "\n",
    "# print(train_component_probs[17:20])\n",
    "# print(train_components[17:20])\n",
    "\n",
    "print(f\"Fitted GMM with {n_components} components\")\n",
    "print(f\"\\nComponent sizes in training set:\")\n",
    "for k in range(n_components):\n",
    "    count = np.sum(train_components == k)\n",
    "    pct = 100 * count / len(train_components)\n",
    "    print(f\"  Component {k}: {count} samples ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDoes the GMM converge? {gmm.converged_}\\nnumber of iterations: {gmm.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00ab2e",
   "metadata": {},
   "source": [
    "## Step 3: Compute Component-Level Claim Probabilities\n",
    "\n",
    "For each component, calculate the empirical claim rate (P(Y=1|component=k)) based on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2688d1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component Risk Profiles:\n",
      "------------------------------------------------------------\n",
      "Component 0:\n",
      "  HARD:\n",
      "    Size: 4001 samples (50.5%)\n",
      "    Claims: 1955.0 (48.9%)\n",
      "    Risk Level (hard): HIGH\n",
      "  SOFT (responsibility-weighted):\n",
      "    Effective size: 4001.0\n",
      "    Effective claims: 1955.0\n",
      "    Claim rate (soft): 48.9%\n",
      "    Risk Level (soft): HIGH\n",
      "\n",
      "Component 1:\n",
      "  HARD:\n",
      "    Size: 2331 samples (29.4%)\n",
      "    Claims: 358.0 (15.4%)\n",
      "    Risk Level (hard): LOW\n",
      "  SOFT (responsibility-weighted):\n",
      "    Effective size: 2331.0\n",
      "    Effective claims: 358.0\n",
      "    Claim rate (soft): 15.4%\n",
      "    Risk Level (soft): LOW\n",
      "\n",
      "Component 2:\n",
      "  HARD:\n",
      "    Size: 1597 samples (20.1%)\n",
      "    Claims: 159.0 (10.0%)\n",
      "    Risk Level (hard): LOW\n",
      "  SOFT (responsibility-weighted):\n",
      "    Effective size: 1597.0\n",
      "    Effective claims: 159.0\n",
      "    Claim rate (soft): 10.0%\n",
      "    Risk Level (soft): LOW\n",
      "\n",
      "Overall training claim rate: 0.3118\n"
     ]
    }
   ],
   "source": [
    "# Calculate claim probability for each component\n",
    "y_train_array = y_train.values \n",
    "component_claim_probs_hard = np.zeros(n_components)\n",
    "component_claim_probs_soft = np.zeros(n_components)\n",
    "\n",
    "print(\"Component Risk Profiles:\")\n",
    "print(\"-\" * 60)\n",
    "for k in range(n_components):\n",
    "    # Hard assignments\n",
    "    mask_hard = (train_components == k)\n",
    "    n_samples_hard = mask_hard.sum()  \n",
    "\n",
    "    if n_samples_hard > 0:\n",
    "        claim_rate_hard = y_train_array[mask_hard].mean()\n",
    "        n_claims_hard = y_train_array[mask_hard].sum()\n",
    "    else:\n",
    "        claim_rate_hard = np.nan\n",
    "        n_claims_hard = 0\n",
    "    \n",
    "    component_claim_probs_hard[k] = claim_rate_hard\n",
    "\n",
    "    # Soft assignments\n",
    "    w_k = train_component_probs[:, k]  # Weights for component k\n",
    "    total_weight = w_k.sum()\n",
    "    \n",
    "    if total_weight > 0:\n",
    "        claim_rate_soft = np.sum(w_k * y_train_array) / total_weight\n",
    "        effective_claims = np.sum(w_k * y_train_array)\n",
    "    else:\n",
    "        claim_rate_soft = np.nan\n",
    "        effective_claims = 0.0\n",
    "    \n",
    "    component_claim_probs_soft[k] = claim_rate_soft\n",
    "    \n",
    "    print(f\"Component {k}:\")\n",
    "    \n",
    "    print(f\"  HARD:\")\n",
    "    print(f\"    Size: {n_samples_hard} samples ({100*n_samples_hard/len(y_train):.1f}%)\")\n",
    "    print(f\"    Claims: {n_claims_hard} ({100*claim_rate_hard:.1f}%)\")\n",
    "    print(f\"    Risk Level (hard): \"f\"{'HIGH' if claim_rate_hard > 0.3 else 'MEDIUM' if claim_rate_hard > 0.2 else 'LOW'}\")\n",
    "\n",
    "    print(f\"  SOFT (responsibility-weighted):\")\n",
    "    print(f\"    Effective size: {total_weight:.1f}\")\n",
    "    print(f\"    Effective claims: {effective_claims:.1f}\")\n",
    "    print(f\"    Claim rate (soft): {100*claim_rate_soft:.1f}%\")\n",
    "    print(f\"    Risk Level (soft): \"\n",
    "          f\"{'HIGH' if claim_rate_soft > 0.3 else 'MEDIUM' if claim_rate_soft > 0.2 else 'LOW'}\\n\")\n",
    "\n",
    "\n",
    "print(f\"Overall training claim rate: {y_train.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2de4458",
   "metadata": {},
   "source": [
    "## Step 4: Make Predictions Using Mixture Model\n",
    "\n",
    "For each customer, we compute:\n",
    "- P(component=k|X) from the GMM (posterior probabilities)\n",
    "- P(Y=1|component=k) from training data (component claim rates)\n",
    "- Overall prediction: P(Y=1|X) = Σ P(component=k|X) × P(Y=1|component=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd131e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated successfully!\n",
      "Train predicted claim rate: 0.505 (actual: 0.3118)\n",
      "Test predicted claim rate: 0.520 (actual: 0.3182)\n"
     ]
    }
   ],
   "source": [
    "def predict_mixture_model(X_scaled, gmm, component_claim_probs):\n",
    "    \"\"\"\n",
    "    Predict claim probability using mixture model approach.\n",
    "    \n",
    "    Args:\n",
    "        X_scaled: Standardized feature matrix\n",
    "        gmm: Fitted GaussianMixture model\n",
    "        component_claim_probs: Array of P(Y=1|component=k) for each component\n",
    "    \n",
    "    Returns:\n",
    "        Array of predicted probabilities P(Y=1|X)\n",
    "    \"\"\"\n",
    "    # Get posterior probabilities P(component=k|X)\n",
    "    component_posteriors = gmm.predict_proba(X_scaled)\n",
    "    \n",
    "    # Compute weighted average: P(Y=1|X) = Σ P(component=k|X) * P(Y=1|component=k)\n",
    "    y_pred_proba = component_posteriors @ component_claim_probs\n",
    "    \n",
    "    return y_pred_proba\n",
    "\n",
    "# Generate predictions for training set (for sanity check)\n",
    "y_train_pred_proba = predict_mixture_model(X_train_scaled, gmm, component_claim_probs_soft)\n",
    "y_train_pred = (y_train_pred_proba >= 0.3).astype(int)\n",
    "\n",
    "# Generate predictions for test set\n",
    "y_test_pred_proba = predict_mixture_model(X_test_scaled, gmm, component_claim_probs_hard)\n",
    "y_test_pred = (y_test_pred_proba >= 0.3).astype(int)\n",
    "\n",
    "print(\"Predictions generated successfully!\")\n",
    "print(f\"Train predicted claim rate: {y_train_pred.mean():.3f} (actual: {y_train.mean():.4f})\")\n",
    "print(f\"Test predicted claim rate: {y_test_pred.mean():.3f} (actual: {y_test.mean():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7272b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_soft_assignments(component_probs, tol=1e-6, n_show=20):\n",
    "    \"\"\"\n",
    "    Analyze non-hard (soft/mixed) component assignments.\n",
    "\n",
    "    Args:\n",
    "        component_probs: array of shape (n_samples, n_components),\n",
    "                         typically from gmm.predict_proba(X)\n",
    "        tol: tolerance for treating a max probability as \"1\"\n",
    "        n_show: how many soft examples to print\n",
    "\n",
    "    Returns:\n",
    "        soft_df: DataFrame with soft assignments and sample indices\n",
    "    \"\"\"\n",
    "    probs = np.asarray(component_probs)\n",
    "    \n",
    "    max_probs = probs.max(axis=1)\n",
    "    is_hard = np.isclose(max_probs, 1.0, atol=tol)\n",
    "    is_soft = ~is_hard\n",
    "\n",
    "    soft_indices = np.where(is_soft)[0]\n",
    "    soft_matrix = probs[is_soft]\n",
    "\n",
    "    print(f\"Total samples: {probs.shape[0]}\")\n",
    "    print(f\"Hard-like assignments: {is_hard.sum()}\")\n",
    "    print(f\"Non-hard (soft/mixed) assignments: {is_soft.sum()}\")\n",
    "\n",
    "    soft_df = pd.DataFrame(\n",
    "        soft_matrix,\n",
    "        columns=[f\"component_{k}\" for k in range(probs.shape[1])]\n",
    "    )\n",
    "    soft_df[\"sample_index\"] = soft_indices\n",
    "\n",
    "    print(\"\\nExamples of non-hard assignments:\")\n",
    "    print(soft_df.head(n_show))\n",
    "\n",
    "    return soft_df\n",
    "\n",
    "soft_df = analyze_soft_assignments(train_component_probs, tol=1e-6, n_show=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef00465",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Model Performance\n",
    "\n",
    "Let's assess how well our mixture model predicts insurance claims on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254bf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"=\" * 70)\n",
    "print(\"MIXTURE MODEL PERFORMANCE ON TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Classification metrics\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 70)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(f\"\\n                 Predicted: No Claim  |  Predicted: Claim\")\n",
    "print(f\"Actual: No Claim      {cm[0,0]:6d}         |      {cm[0,1]:6d}\")\n",
    "print(f\"Actual: Claim         {cm[1,0]:6d}         |      {cm[1,1]:6d}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test, y_test_pred, target_names=['No Claim', 'Claim']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04611a2d",
   "metadata": {},
   "source": [
    "### Visualize ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bf999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Mixture Model', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4c341",
   "metadata": {},
   "source": [
    "### Analyze Component Assignments on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how test samples are distributed across components\n",
    "test_components = gmm.predict(X_test_scaled)\n",
    "test_component_probs = gmm.predict_proba(X_test_scaled)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST SET COMPONENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_test_array = y_test.values\n",
    "\n",
    "for k in range(n_components):\n",
    "    mask = (test_components == k)\n",
    "    n_samples = mask.sum()\n",
    "    actual_claim_rate = y_test_array[mask].mean()\n",
    "    predicted_claim_rate = component_claim_probs_hard[k]\n",
    "    \n",
    "    print(f\"\\nComponent {k}:\")\n",
    "    print(f\"  Test samples: {n_samples} ({100*n_samples/len(y_test):.1f}%)\")\n",
    "    print(f\"  Predicted claim prob (from train): {predicted_claim_rate:.3f}\")\n",
    "    print(f\"  Actual claim rate (in test): {actual_claim_rate:.3f}\")\n",
    "    print(f\"  Difference: {abs(actual_claim_rate - predicted_claim_rate):.3f}\")\n",
    "    \n",
    "    # Show prediction confidence\n",
    "    avg_confidence = test_component_probs[mask, k].mean()\n",
    "    print(f\"  Average membership confidence: {avg_confidence:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e16ced",
   "metadata": {},
   "source": [
    "### Visualize Predicted vs Actual Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a830a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of predicted probabilities by actual outcome\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of predicted probabilities\n",
    "axes[0].hist(y_test_pred_proba[y_test == 0], bins=30, alpha=0.6, label='No Claim (Actual)', color='blue', edgecolor='black')\n",
    "axes[0].hist(y_test_pred_proba[y_test == 1], bins=30, alpha=0.6, label='Claim (Actual)', color='red', edgecolor='black')\n",
    "axes[0].axvline(0.5, color='green', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "axes[0].set_xlabel('Predicted Probability of Claim', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Predicted Probabilities', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot by actual outcome\n",
    "test_df = pd.DataFrame({\n",
    "    'Predicted_Prob': y_test_pred_proba,\n",
    "    'Actual_Outcome': y_test\n",
    "})\n",
    "sns.boxplot(x='Actual_Outcome', y='Predicted_Prob', data=test_df, ax=axes[1])\n",
    "axes[1].set_xlabel('Actual Outcome', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Probability', fontsize=12)\n",
    "axes[1].set_title('Predicted Probabilities by Actual Outcome', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xticklabels(['No Claim', 'Claim'])\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
